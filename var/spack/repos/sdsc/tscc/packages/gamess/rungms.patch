--- a/rungms	2022-04-15 01:02:37.000000000 -0700
+++ b/rungms	2023-03-01 16:40:01.293308960 -0800
@@ -80,10 +80,14 @@
 #       both Sun Grid Engine (SGE), and Portable Batch System (PBS).
 #       See also a very old LoadLeveler "ll-gms" for some IBM systems.
 #
-set TARGET=sockets
-set SCR=~/gamess/restart
-set USERSCR=~/gamess/restart
-set GMSPATH=~/gamess
+set TARGET=mpi
+set SCR=$PWD
+set USERSCR=$PWD
+if ( ! $?GAMESS_SCR )  then
+  set SCR=$PWD
+else
+  set SCR=$GAMESS_SCR
+endif
 #
 # Get any MDI-related options and remove them from the argument list
 #
@@ -164,7 +168,8 @@
 #      on every assigned node (if not, modify scheduler's prolog script).
 #      The SCHED variable, and scheduler assigned work space, is used
 #      below only in the MPI section.  See that part for more info.
-                      set SCHED=none
+set SCHED=none
+if ($?SLURM_JOBID)    set SCHED=SLURM
 if ($?PBS_O_LOGNAME)  set SCHED=PBS
 if ($?SGE_O_LOGNAME)  set SCHED=SGE
 if ($SCHED == SGE) then
@@ -173,15 +178,24 @@
    uniq $TMPDIR/machines
 endif
 if ($SCHED == PBS) then
-   #    our ISU clusters have different names for local working disk space.
-   if ($?TMPDIR) then
-      set SCR=$TMPDIR
-   else
-      set SCR=/scratch/$PBS_JOBID
-   endif
    echo "PBS has assigned the following compute nodes to this run:"
    uniq $PBS_NODEFILE
 endif
+if ($SCHED == SLURM) then
+   echo "SLURM has assigned the following compute nodes to this run:"
+   setenv GMS_SLURM_NODES `scontrol show hostname`
+   setenv PBS_NUM_PPN $SLURM_NTASKS_PER_NODE
+   set PBS_NODEFILE=$$.hosts
+   foreach host ( $GMS_SLURM_NODES )
+       set cpu = 1
+       while ( $cpu <=  $PBS_NUM_PPN )
+           echo $host >> $PBS_NODEFILE
+           @ cpu ++
+       end
+   end
+   uniq $PBS_NODEFILE
+endif
+
 #
 echo "Available scratch disk space (Kbyte units) at beginning of the job is"
 df -k $SCR
@@ -641,7 +655,7 @@
    #      we'll pass in a "processers per node" value, that is,
    #      all nodes are presumed to have equal numbers of cores.
    #
-   set PPN=$4
+   set PPN=$PBS_NUM_PPN
    #
    #      Allow for compute process and data servers (one pair per core)
    #      note that NCPUS = #cores, and NPROCS = #MPI processes
@@ -656,37 +670,11 @@
    #          this will have directories like include/lib/bin below it.
    #       3. a bit lower, perhaps specify your ifort path information.
    #
-   set DDI_MPI_CHOICE=impi
-   #
-   #        ISU's various clusters have various iMPI paths, in this order:
-   #              dynamo/chemphys2011/exalted/bolt/CyEnce/CJ
-   if ($DDI_MPI_CHOICE == impi) then
-      #-- DDI_MPI_ROOT=/opt/intel/impi/3.2
-      #-- DDI_MPI_ROOT=/share/apps/intel/impi/4.0.1.007/intel64
-      #-- DDI_MPI_ROOT=/share/apps/intel/impi/4.0.2.003/intel64
-      #-- DDI_MPI_ROOT=/share/apps/mpi/impi/intel64
-      set DDI_MPI_ROOT=/shared/intel/impi/4.1.0.024/intel64
-      #-- DDI_MPI_ROOT=/share/apps/mpi/impi/intel64
-   endif
-   #
-   #        ISU's various clusters have various MVAPICH2 paths, in this order:
-   #              dynamo/exalted/bolt/thebunny/CJ
-   if ($DDI_MPI_CHOICE == mvapich2) then
-      #-- DDI_MPI_ROOT=/share/apps/mpi/mvapich2-1.9a2-generic
-      #-- DDI_MPI_ROOT=/share/apps/mpi/mvapich2-1.9a2-qlc
-      #-- DDI_MPI_ROOT=/share/apps/mpi/mvapich2-1.9-generic-gnu
-      #-- DDI_MPI_ROOT=/share/apps/mpi/mvapich2-2.0a-generic
-      set DDI_MPI_ROOT=/share/apps/mpi/mvapich2-2.1a-mlnx
-   endif
-   #
-   #        ISU's various clusters have various openMPI paths
-   #          examples are our bolt/CyEnce clusters
-   if ($DDI_MPI_CHOICE == openmpi) then
-      #-- DDI_MPI_ROOT=/share/apps/mpi/openmpi-1.6.4-generic
-      set DDI_MPI_ROOT=/shared/openmpi-1.6.4/intel-13.0.1
-   endif
    #
    #   MPICH/MPICH2
+   set DDI_MPI_CHOICE=ROLL_MPITYPE
+   set DDI_MPI_ROOT=ROLLMPI
+
    if ($DDI_MPI_CHOICE == mpich) then
       set DDI_MPI_ROOT=/share/apps/share/mpi/mpich-3.1.3-generic-gnu
    endif
@@ -760,7 +748,7 @@
          set NNODES=`wc -l $HOSTFILE`
          set NNODES=$NNODES[1]
       endif
-      if ($SCHED == PBS) then
+      if ($SCHED == PBS || $SCHED == SLURM) then
          uniq $PBS_NODEFILE $HOSTFILE
          set NNODES=`wc -l $HOSTFILE`
          set NNODES=$NNODES[1]
@@ -871,8 +859,6 @@
       setenv I_MPI_DEBUG 0
       setenv I_MPI_STATS 0
       #              next two select highest speed mode of an Infiniband
-      setenv I_MPI_FABRICS dapl
-      setenv I_MPI_DAT_LIBRARY libdat2.so
       # Force use of "shared memory copy" large message transfer mechanism
       # The "direct" mechanism was introduced and made default for IPS 2017,
       # and makes GAMESS hang when DD_GSum() is called. See IPS 2017 release notes
@@ -894,7 +880,7 @@
    #      similar tunings for MVAPICH2 are
    if ($DDI_MPI_CHOICE == mvapich2) then
       set echo
-      setenv MV2_USE_BLOCKING 1
+#     setenv MV2_USE_BLOCKING 1
       setenv MV2_ENABLE_AFFINITY 0
       unset echo
    endif
@@ -1043,15 +1029,15 @@
          unset echo
       endif
       set echo
-      mpiexec.hydra -f $PROCFILE -n $NPROCS \
+      mpirun -hostfile $PROCFILE -np $NPROCS \
             $GMSPATH/gamess.$VERNO.x < /dev/null
       unset echo
       breaksw
 
    case orte:
       set echo
-      orterun -np $NPROCS --npernode $PPN2 \
-              $GMSPATH/gamess.$VERNO.x < /dev/null
+      mpirun --oversubscribe -hostfile $HOSTFILE -np $NPROCS --npernode $PPN2 \
+         $GMSPATH/gamess.$VERNO.x < /dev/null
       unset echo
       breaksw
    #
@@ -1136,7 +1122,7 @@
          set NNODES=`wc -l $HOSTFILE`
          set NNODES=$NNODES[1]
       endif
-      if ($SCHED == PBS) then
+      if ($SCHED == PBS || $SCHED == SLURM)) then
          uniq $PBS_NODEFILE $HOSTFILE
          set NNODES=`wc -l $HOSTFILE`
          set NNODES=$NNODES[1]
