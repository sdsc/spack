slurmstepd: error: Unable to create TMPDIR [/scratch/spack_gpu/job_21909574]: Permission denied
slurmstepd: error: Setting TMPDIR to /tmp
1682365448 21909617 605e13a84130f3552b2d564ef8d38693  /home/spack_gpu/software/namd/namd@2.14.sh afterok:21909574

#!/usr/bin/env bash

#SBATCH --job-name=namd@2.14
#SBATCH --account=use300
#SBATCH --reservation=rocky8u7_testing
#SBATCH --partition=ind-gpu-shared
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=93G
#SBATCH --gpus=1
#SBATCH --time=01:00:00
#SBATCH --output=%x.o%j.%N

declare -xr LOCAL_TIME="$(date +'%Y%m%dT%H%M%S%z')"
declare -xir UNIX_TIME="$(date +'%s')"

declare -xr LOCAL_SCRATCH_DIR="/scratch/${USER}/job_${SLURM_JOB_ID}"
declare -xr TMPDIR="${LOCAL_SCRATCH_DIR}"

declare -xr SYSTEM_NAME='expanse'

declare -xr SPACK_VERSION='0.17.3'
declare -xr SPACK_INSTANCE_NAME='gpu'
declare -xr SPACK_INSTANCE_VERSION='b'
declare -xr SPACK_INSTANCE_DIR="/cm/shared/apps/spack/${SPACK_VERSION}/${SPACK_INSTANCE_NAME}/${SPACK_INSTANCE_VERSION}"

declare -xr SLURM_JOB_SCRIPT="$(scontrol show job ${SLURM_JOB_ID} | awk -F= '/Command=/{print $2}')"
declare -xr SLURM_JOB_MD5SUM="$(md5sum ${SLURM_JOB_SCRIPT})"

declare -xr SCHEDULER_MODULE='slurm'

echo "${UNIX_TIME} ${SLURM_JOB_ID} ${SLURM_JOB_MD5SUM} ${SLURM_JOB_DEPENDENCY}" 
echo ""

cat "${SLURM_JOB_SCRIPT}"

module purge
module load "${SCHEDULER_MODULE}"
module list
. "${SPACK_INSTANCE_DIR}/share/spack/setup-env.sh"


# smp is not added to CHARMARCH as should be expected, despite charmpp being built with verbs +smp; one of these packages may need to be fixed, or both?

#==> namd: Executing phase: 'edit'
#==> [2022-12-16-09:03:05.607692] '/home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/gcc' '-dumpversion'
#==> [2022-12-16-09:03:05.633662] Copying arch/Linux-x86_64.base to arch/linux-x86_64.base
#==> [2022-12-16-09:03:05.634121] Copying arch/Linux-x86_64.fftw3 to arch/linux-x86_64.fftw3
#==> [2022-12-16-09:03:05.634455] Copying arch/Linux-x86_64.tcl to arch/linux-x86_64.tcl
#==> [2022-12-16-09:03:05.634776] FILTER FILE: arch/linux-x86_64.tcl [replacing "-ltcl8\.5"]
#==> [2022-12-16-09:03:05.635607] Copying arch/Linux-x86_64.cuda to arch/linux-x86_64.cuda
#==> [2022-12-16-09:03:05.635913] FILTER FILE: arch/linux-x86_64.cuda [replacing "^CUDADIR=.*$"]
#==> [2022-12-16-09:03:05.636624] './config' 'linux-x86_64-spack' '--charm-base' '/home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/charmpp-6.10.2-nlla43jibnyqgezncn7q44cpzoddjlxe' '--with-fftw3' '--fftw-prefix' '/home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/fftw-3.3.10-7ahyh5v5twqbte3ye62ukz55jqgwheuk' '--with-tcl' '--tcl-prefix' '/home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/tcl-8.5.9-6xpusrkzpnmqhi3uw6anyn7unxlsqvty' '--with-cuda' '--cuda-prefix' '/home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/cuda-11.3.1-gonjgx5gtwrgpnixvmchcaozt6bv2ykl'
#
#Selected arch file arch/linux-x86_64-spack.arch contains:
#
#NAMD_ARCH = linux-x86_64
#CHARMARCH = verbs-linux-x86_64
#CXX = /home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/g++ -std=c++11
#CXXOPTS = -m64 -O3 -fexpensive-optimizations                                         -ffast-math -lpthread -march=cascadelake -mtune=cascadelake
#CC = /home/mkandes/cm/shared/apps/spack/0.17.3/gpu/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/gcc
#COPTS = -m64 -O3 -fexpensive-optimizations                                         -ffast-math -lpthread -march=cascadelake -mtune=cascadelake
#
#ERROR: Non-SMP Charm++ arch  is not compatible with CUDA NAMD.
#ERROR: CUDA builds require non-MPI SMP or multicore Charm++ arch for reasonable performance.
#
#Consider ucx-smp or verbs-smp (InfiniBand), gni-smp (Cray), or multicore (single node).

declare -xr SPACK_PACKAGE='namd@2.14'
declare -xr SPACK_COMPILER='gcc@10.2.0'
declare -xr SPACK_VARIANTS='+cuda cuda_arch=70,80 interface=tcl'
declare -xr SPACK_DEPENDENCIES="^charmpp@6.10.2/$(spack find --format '{hash:7}' charmpp@6.10.2 % ${SPACK_COMPILER} backend=multicore) ^fftw@3.3.10/$(spack find --format '{hash:7}' fftw@3.3.10 % ${SPACK_COMPILER} ~mpi ~openmp) ^tcl@8.5.9/$(spack find --format '{hash:7}' tcl@8.5.9 % ${SPACK_COMPILER})"
declare -xr SPACK_SPEC="${SPACK_PACKAGE} % ${SPACK_COMPILER} ${SPACK_VARIANTS} ${SPACK_DEPENDENCIES}"

printenv

spack config get compilers
spack config get config  
spack config get mirrors
spack config get modules
spack config get packages
spack config get repos
spack config get upstreams

time -p spack spec --long --namespaces --types namd@2.14 % gcc@10.2.0 +cuda cuda_arch=70,80 interface=tcl "^charmpp@6.10.2/$(spack find --format '{hash:7}' charmpp@6.10.2 % ${SPACK_COMPILER} backend=multicore) ^fftw@3.3.10/$(spack find --format '{hash:7}' fftw@3.3.10 % ${SPACK_COMPILER} ~mpi ~openmp) ^tcl@8.5.9/$(spack find --format '{hash:7}' tcl@8.5.9 % ${SPACK_COMPILER})"
if [[ "${?}" -ne 0 ]]; then
  echo 'ERROR: spack concretization failed.'
  exit 1
fi

time -p spack install --jobs "${SLURM_CPUS_PER_TASK}" --fail-fast --yes-to-all namd@2.14 % gcc@10.2.0 +cuda cuda_arch=70,80 interface=tcl "^charmpp@6.10.2/$(spack find --format '{hash:7}' charmpp@6.10.2 % ${SPACK_COMPILER} backend=multicore) ^fftw@3.3.10/$(spack find --format '{hash:7}' fftw@3.3.10 % ${SPACK_COMPILER} ~mpi ~openmp) ^tcl@8.5.9/$(spack find --format '{hash:7}' tcl@8.5.9 % ${SPACK_COMPILER})"
if [[ "${?}" -ne 0 ]]; then
  echo 'ERROR: spack install failed.'
  exit 1
fi

#spack module lmod refresh --delete-tree -y

#sbatch --dependency="afterok:${SLURM_JOB_ID}" ''

sleep 30

Currently Loaded Modules:
  1) slurm/expanse/21.08.8

 

LD_LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64
LS_COLORS=rs=0:di=38;5;33:ln=38;5;51:mh=00:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=01;05;37;41:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;40:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.zst=38;5;9:*.tzst=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.wim=38;5;9:*.swm=38;5;9:*.dwm=38;5;9:*.esd=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.mjpg=38;5;13:*.mjpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.m4a=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.oga=38;5;45:*.opus=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
LOCAL_SCRATCH_DIR=/scratch/spack_gpu/job_21909617
SLURM_NODEID=0
SLURM_TASK_PID=2525740
__LMOD_REF_COUNT_PATH=/cm/shared/apps/slurm/current/sbin:1;/cm/shared/apps/slurm/current/bin:1;/cm/shared/apps/spack/0.17.3/gpu/b/bin:1;/home/spack_gpu/.local/bin:1;/home/spack_gpu/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1
_ModuleTable002_=b2R1bGVmaWxlcyIsIi9jbS9zaGFyZWQvYXBwcy9hY2Nlc3MvbW9kdWxlZmlsZXMiLCIvY20vc2hhcmVkL2FwcHMveHNlZGUvbW9kdWxlZmlsZXMiLCIvZXRjL21vZHVsZWZpbGVzIiwiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcyIsIi91c3Ivc2hhcmUvTW9kdWxlcy9tb2R1bGVmaWxlcyIsIi9jbS9zaGFyZWQvbW9kdWxlZmlsZXMiLH0sWyJzeXN0ZW1CYXNlTVBBVEgiXT0iL2NtL2xvY2FsL21vZHVsZWZpbGVzOi9jbS9zaGFyZWQvYXBwcy9hY2Nlc3MvbW9kdWxlZmlsZXM6L2NtL3NoYXJlZC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzOi9jbS9zaGFyZWQvbW9kdWxlZmlsZXM6L2V0Yy9tb2R1bGVmaWxlczovdXNyL3NoYXJlL21vZHVsZWZpbGVzOi91c3Ivc2hhcmUvTW9kdWxlcy9t
SSH_CONNECTION=198.202.100.14 58876 198.202.100.14 22
SPACK_PYTHON=/usr/bin/python3
SLURM_PRIO_PROCESS=0
SLURM_SUBMIT_DIR=/home/spack_gpu/software/namd
HISTCONTROL=ignoredups
COMPILER_MODULE=gcc/10.2.0
HOSTNAME=exp-15-57
LMOD_SYSTEM_DEFAULT_MODULES=DefaultModules
SLURM_JOB_DEPENDENCY=afterok:21909574
OLDPWD=/home/spack_gpu
__LMOD_REF_COUNT__LMFILES_=/cm/local/modulefiles/slurm/expanse/21.08.8:1
SLURM_CPUS_PER_TASK=10
CUDA_MODULE=cuda/11.2.2
SPACK_DEPENDENCIES=^charmpp@6.10.2/ldvm3k4 ^fftw@3.3.10/7ahyh5v ^tcl@8.5.9/6xpusrk
ENVIRONMENT=BATCH
SPACK_VERSION=0.17.3
ROCR_VISIBLE_DEVICES=0
SLURM_PROCID=0
SPACK_INSTANCE_VERSION=b
SLURM_JOB_GID=11491
SPACK_VARIANTS=+cuda cuda_arch=70,80 interface=tcl
SPACK_INSTANCE_DIR=/cm/shared/apps/spack/0.17.3/gpu/b
__LMOD_REF_COUNT_LD_LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:1;/cm/shared/apps/slurm/current/lib64:1
SLURM_JOB_SCRIPT=/home/spack_gpu/software/namd/namd@2.14.sh
SLURMD_NODENAME=exp-15-57
SLURM_TASKS_PER_NODE=1
S_COLORS=auto
which_declare=declare -f
SPACK_COMPILER=gcc@10.2.0
SLURM_JOB_RESERVATION=rocky8u7_testing
XDG_SESSION_ID=91618
USER=spack_gpu
SLURM_NNODES=1
SLURM_NTASKS_PER_NODE=1
__LMOD_REF_COUNT_MODULEPATH=/cm/shared/apps/spack/0.17.3/gpu/b/share/spack/lmod/linux-rocky8-x86_64/Core:1;/cm/local/modulefiles:1;/cm/shared/apps/access/modulefiles:1;/cm/shared/apps/xsede/modulefiles:1;/etc/modulefiles:1;/usr/share/modulefiles:1;/usr/share/Modules/modulefiles:1;/cm/shared/modulefiles:1
__LMOD_REF_COUNT_LOADEDMODULES=slurm/expanse/21.08.8:1
SLURM_GPUS=1
PWD=/home/spack_gpu/software/namd
ENABLE_LMOD=1
SLURM_JOB_NODELIST=exp-15-57
HOME=/home/spack_gpu
SLURM_CLUSTER_NAME=expanse
LMOD_COLORIZE=yes
LOCAL_TIME=20230424T124408-0700
SLURM_NODELIST=exp-15-57
SLURM_GPUS_ON_NODE=1
SSH_CLIENT=198.202.100.14 58876 22
LMOD_VERSION=8.2.4
CPATH=/cm/shared/apps/slurm/current/include
SLURM_NTASKS=1
LMOD_SETTARG_CMD=:
SLURM_JOB_CPUS_PER_NODE=10
BASH_ENV=/usr/share/lmod/lmod/init/bash
SLURM_TOPOLOGY_ADDR=exp-15-57
SLURM_WORKING_CLUSTER=expanse:mgr1:6817:9472:109
SLURM_JOB_MD5SUM=605e13a84130f3552b2d564ef8d38693  /home/spack_gpu/software/namd/namd@2.14.sh
SPACK_PACKAGE=namd@2.14
__LMOD_REF_COUNT_LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:1;/cm/shared/apps/slurm/current/lib64:1
SLURM_JOB_NAME=namd@2.14
TMPDIR=/scratch/spack_gpu/job_21909617
LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64
SLURM_JOB_GPUS=0
LMOD_sys=Linux
SLURM_JOBID=21909617
SYSTEM_NAME=expanse
_ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtzbHVybT17WyJmbiJdPSIvY20vbG9jYWwvbW9kdWxlZmlsZXMvc2x1cm0vZXhwYW5zZS8yMS4wOC44IixbImZ1bGxOYW1lIl09InNsdXJtL2V4cGFuc2UvMjEuMDguOCIsWyJsb2FkT3JkZXIiXT0xLHByb3BUPXt9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09InNsdXJtIix9LH0sbXBhdGhBPXsiL2NtL3NoYXJlZC9hcHBzL3NwYWNrLzAuMTcuMy9ncHUvYi9zaGFyZS9zcGFjay9sbW9kL2xpbnV4LXJvY2t5OC14ODZfNjQvQ29yZSIsIi9jbS9sb2NhbC9t
SLURM_CONF=/cm/shared/apps/slurm/var/etc/expanse/slurm.conf
LOADEDMODULES=slurm/expanse/21.08.8
__LMOD_REF_COUNT_MANPATH=/cm/shared/apps/slurm/current/man:2;/usr/share/lmod/lmod/share/man:1;/usr/local/share/man:1;/usr/share/man:1;/cm/local/apps/environment-modules/current/share/man:1
_ModuleTable003_=b2R1bGVmaWxlcyIsfQ==
SLURM_NODE_ALIASES=(null)
LMOD_ROOT=/usr/share/lmod
SLURM_JOB_QOS=ind-gpu-shared-normal
SLURM_TOPOLOGY_ADDR_PATTERN=node
SSH_TTY=/dev/pts/1399
MAIL=/var/spool/mail/spack_gpu
SLURM_CPUS_ON_NODE=10
LMOD_arch=x86_64
SLURM_JOB_NUM_NODES=1
__Init_Default_Modules=1
CMD_WLM_CLUSTER_NAME=expanse
SLURM_MEM_PER_NODE=95232
SPACK_ROOT=/cm/shared/apps/spack/0.17.3/gpu/b
TERM=xterm-256color
SHELL=/bin/bash
_ModuleTable_Sz_=3
SLURM_JOB_UID=527835
__LMOD_REF_COUNT_CPATH=/cm/shared/apps/slurm/current/include:1
SLURM_JOB_PARTITION=ind-gpu-shared
SPACK_INSTANCE_NAME=gpu
SLURM_JOB_USER=spack_gpu
CUDA_VISIBLE_DEVICES=0
SLURM_NPROCS=1
SHLVL=3
SLURM_SUBMIT_HOST=exp-15-57
UNIX_TIME=1682365448
SLURM_JOB_ACCOUNT=use300
MANPATH=/cm/shared/apps/slurm/current/man:/usr/share/lmod/lmod/share/man::/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man
SPACK_LD_LIBRARY_PATH=/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/cuda-11.2.2-blza2psofa3wr2zumqrnh4je2f7ze3mx/lib64:/cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/lib64:/cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64
LMOD_PREPEND_BLOCK=normal
MODULEPATH=/cm/shared/apps/spack/0.17.3/gpu/b/share/spack/lmod/linux-rocky8-x86_64/Core:/cm/local/modulefiles:/cm/shared/apps/access/modulefiles:/cm/shared/apps/xsede/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/share/Modules/modulefiles:/cm/shared/modulefiles
SLURM_GTIDS=0
LOGNAME=spack_gpu
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/527835/bus
XDG_RUNTIME_DIR=/run/user/527835
MODULEPATH_ROOT=/usr/share/modulefiles
PATH=/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/shared/apps/spack/0.17.3/gpu/b/bin:/home/spack_gpu/.local/bin:/home/spack_gpu/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
SLURM_JOB_ID=21909617
_LMFILES_=/cm/local/modulefiles/slurm/expanse/21.08.8
MODULESHOME=/usr/share/lmod/lmod
SCHEDULER_MODULE=slurm
LMOD_SETTARG_FULL_SUPPORT=no
HISTSIZE=-1
LMOD_PKG=/usr/share/lmod/lmod
SPACK_SPEC=namd@2.14 % gcc@10.2.0 +cuda cuda_arch=70,80 interface=tcl ^charmpp@6.10.2/ldvm3k4 ^fftw@3.3.10/7ahyh5v ^tcl@8.5.9/6xpusrk
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
SLURM_LOCALID=0
GPU_DEVICE_ORDINAL=0
LESSOPEN=||/usr/bin/lesspipe.sh %s
LMOD_FULL_SETTARG_SUPPORT=no
LMOD_DIR=/usr/share/lmod/lmod/libexec
BASH_FUNC_which%%=() {  ( alias;
 eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot "$@"
}
BASH_FUNC_module%%=() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}
BASH_FUNC_spack%%=() {  : this is a shell function from: /cm/shared/apps/spack/0.17.3/gpu/b/share/spack/setup-env.sh;
 : the real spack script is here: /cm/shared/apps/spack/0.17.3/gpu/b/bin/spack;
 _spack_shell_wrapper "$@";
 return $?
}
BASH_FUNC__spack_shell_wrapper%%=() {  for var in LD_LIBRARY_PATH DYLD_LIBRARY_PATH DYLD_FALLBACK_LIBRARY_PATH;
 do
 eval "if [ -n \"\${${var}-}\" ]; then export SPACK_$var=\${${var}}; fi";
 done;
 if [ -n "${ZSH_VERSION:-}" ]; then
 emulate -L sh;
 fi;
 _sp_flags="";
 while [ ! -z ${1+x} ] && [ "${1#-}" != "${1}" ]; do
 _sp_flags="$_sp_flags $1";
 shift;
 done;
 if [ -n "$_sp_flags" ] && [ "${_sp_flags#*h}" != "${_sp_flags}" ] || [ "${_sp_flags#*V}" != "${_sp_flags}" ]; then
 command spack $_sp_flags "$@";
 return;
 fi;
 _sp_subcommand="";
 if [ ! -z ${1+x} ]; then
 _sp_subcommand="$1";
 shift;
 fi;
 case $_sp_subcommand in 
 "cd")
 _sp_arg="";
 if [ -n "$1" ]; then
 _sp_arg="$1";
 shift;
 fi;
 if [ "$_sp_arg" = "-h" ] || [ "$_sp_arg" = "--help" ]; then
 command spack cd -h;
 else
 LOC="$(spack location $_sp_arg "$@")";
 if [ -d "$LOC" ]; then
 cd "$LOC";
 else
 return 1;
 fi;
 fi;
 return
 ;;
 "env")
 _sp_arg="";
 if [ -n "$1" ]; then
 _sp_arg="$1";
 shift;
 fi;
 if [ "$_sp_arg" = "-h" ] || [ "$_sp_arg" = "--help" ]; then
 command spack env -h;
 else
 case $_sp_arg in 
 activate)
 _a=" $@";
 if [ -z ${1+x} ] || [ "${_a#* --sh}" != "$_a" ] || [ "${_a#* --csh}" != "$_a" ] || [ "${_a#* -h}" != "$_a" ] || [ "${_a#* --help}" != "$_a" ]; then
 command spack env activate "$@";
 else
 stdout="$(command spack $_sp_flags env activate --sh "$@")" || return;
 eval "$stdout";
 fi
 ;;
 deactivate)
 _a=" $@";
 if [ "${_a#* --sh}" != "$_a" ] || [ "${_a#* --csh}" != "$_a" ]; then
 command spack env deactivate "$@";
 else
 if [ -n "$*" ]; then
 command spack env deactivate -h;
 else
 stdout="$(command spack $_sp_flags env deactivate --sh)" || return;
 eval "$stdout";
 fi;
 fi
 ;;
 *)
 command spack env $_sp_arg "$@"
 ;;
 esac;
 fi;
 return
 ;;
 "load" | "unload")
 _a=" $@";
 if [ "${_a#* --sh}" != "$_a" ] || [ "${_a#* --csh}" != "$_a" ] || [ "${_a#* -h}" != "$_a" ] || [ "${_a#* --list}" != "$_a" ] || [ "${_a#* --help}" != "$_a" ]; then
 command spack $_sp_flags $_sp_subcommand "$@";
 else
 stdout="$(command spack $_sp_flags $_sp_subcommand --sh "$@")" || return;
 eval "$stdout";
 fi
 ;;
 *)
 command spack $_sp_flags $_sp_subcommand "$@"
 ;;
 esac
}
BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd "$@")
}
_=/usr/bin/printenv
compilers:
- compiler:
    spec: gcc@8.5.0
    paths:
      cc: /usr/bin/gcc
      cxx: /usr/bin/g++
      f77: /usr/bin/gfortran
      fc: /usr/bin/gfortran
    flags:
      cflags: -O2 -march=native
      cxxflags: -O2 -march=native
      fflags: -O2 -march=native
    operating_system: rocky8
    target: x86_64
    modules: []
    environment: {}
    extra_rpaths: []
- compiler:
    spec: gcc@10.2.0
    paths:
      cc: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/gcc
      cxx: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/g++
      f77: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/gfortran
      fc: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/gcc-10.2.0-i62tgsoexc6ya4h7urwhriiujk22nrnj/bin/gfortran
    flags:
      cflags: -O2 -march=native
      cxxflags: -O2 -march=native
      fflags: -O2 -march=native
    operating_system: rocky8
    target: x86_64
    modules: []
    environment: {}
    extra_rpaths: []
- compiler:
    spec: intel@19.1.3.304
    paths:
      cc: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/intel-19.1.3.304-vecir2bnonslbnjniwcxx6n5vfyeg4yf/bin/icc
      cxx: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/intel-19.1.3.304-vecir2bnonslbnjniwcxx6n5vfyeg4yf/bin/icpc
      f77: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/intel-19.1.3.304-vecir2bnonslbnjniwcxx6n5vfyeg4yf/bin/ifort
      fc: /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/intel-19.1.3.304-vecir2bnonslbnjniwcxx6n5vfyeg4yf/bin/ifort
    flags:
      cflags: -O2 -march=native
      cxxflags: -O2 -march=native
      fflags: -O2 -march=native
    operating_system: rocky8
    target: x86_64
    modules: []
    environment: {}
    extra_rpaths: []
config:
  install_tree:
    root: $spack/opt/spack
    projections:
      all: ${ARCHITECTURE}/${COMPILERNAME}-${COMPILERVER}/${PACKAGE}-${VERSION}-${HASH}
  template_dirs:
  - $spack/share/spack/templates

  # Temporary locations Spack can try to use for builds.
  #
  # Recommended options are given below.
  #
  # Builds can be faster in temporary directories on some (e.g., HPC) systems.
  # Specifying `$tempdir` will ensure use of the default temporary directory
  # (i.e., ``$TMP` or ``$TMPDIR``).
  #
  # Another option that prevents conflicts and potential permission issues is
  # to specify `$user_cache_path/stage`, which ensures each user builds in their
  # home directory.
  #
  # A more traditional path uses the value of `$spack/var/spack/stage`, which
  # builds directly inside Spack's instance without staging them in a
  # temporary space.  Problems with specifying a path inside a Spack instance
  # are that it precludes its use as a system package and its ability to be
  # pip installable.
  #
  # In any case, if the username is not already in the path, Spack will append
  # the value of `$user` in an attempt to avoid potential conflicts between
  # users in shared temporary spaces.
  #
  # The build stage can be purged with `spack clean --stage` and
  # `spack clean -a`, so it is important that the specified directory uniquely
  # identifies Spack staging to avoid accidentally wiping out non-Spack work.
  build_stage:
  - $tempdir/$user/spack-stage
  - $user_cache_path/stage
  # - $spack/var/spack/stage

  # Directory in which to run tests and store test results.
  # Tests will be stored in directories named by date/time and package
  # name/hash.
  test_stage: $user_cache_path/test
  source_cache: $spack/var/spack/cache
  misc_cache: $user_cache_path/cache
  connect_timeout: 10
  verify_ssl: true
  suppress_gpg_warnings: false
  install_missing_compilers: false
  checksum: true
  deprecated: false
  dirty: false
  build_language: C
  locks: true
  url_fetch_method: urllib
  ccache: false
  concretizer: clingo
  db_lock_timeout: 3
  package_lock_timeout: null
  shared_linking: rpath
  allow_sgid: true
  terminal_title: false
  debug: false
  build_jobs: 10
mirrors:
  spack-public: https://mirror.spack.io
modules:
  default:
    'enable:':
    - lmod
    lmod:
      core_compilers:
      - gcc@8.5.0
      hierarchy:
      - mpi
      hash_length: 0
      blacklist_implicits: true
      naming_scheme: '{name}/{version}/{hash:7}'
      projections:
        all: '{name}/{version}/{hash:7}'
      all:
        suffixes:
          +openmp: omp
          threads=openmp: omp
          +ipl64: i64
      cuda:
        environment:
          prepend_path:
            PATH: /cm/local/apps/cuda/libs/current/bin
            LD_LIBRARY_PATH: /cm/local/apps/cuda/libs/current/lib64
          set:
            CUDATOOLKIT_HOME: /cm/local/apps/cuda
      intel:
        environment:
          set:
            INTEL_LICENSE_FILE: 40000@elprado.sdsc.edu:40200@elprado.sdsc.edu
      petsc:
        suffixes:
          +complex: cmplx
      slepc:
        suffixes:
          ^petsc +complex: cmplx
      ^cuda:
        autoload: direct
      ^python:
        autoload: direct
      ^ucx:
        autoload: direct
  prefix_inspections:
    lib:
    - LD_LIBRARY_PATH
    lib64:
    - LD_LIBRARY_PATH
    bin:
    - PATH
    man:
    - MANPATH
    share/man:
    - MANPATH
    share/aclocal:
    - ACLOCAL_PATH
    lib/pkgconfig:
    - PKG_CONFIG_PATH
    lib64/pkgconfig:
    - PKG_CONFIG_PATH
    share/pkgconfig:
    - PKG_CONFIG_PATH
    ? ''
    : - CMAKE_PREFIX_PATH

  # These are configurations for the module set named "default"
packages:
  lmod:
    externals:
    - spec: lmod@8.2.4
      prefix: /usr
    buildable: false
  lustre:
    externals:
    - spec: lustre@2.15.2
      prefix: /usr
    buildable: false
  openssl:
    externals:
    - spec: openssl@1.1.1k
      prefix: /usr
    buildable: false
  rdma-core:
    externals:
    - spec: rdma-core@43.0
      prefix: /usr
    buildable: false
  slurm:
    externals:
    - spec: slurm@21.08.8
      prefix: /cm/shared/apps/slurm/21.08.8
    buildable: false
  librsvg:
    externals:
    - spec: librsvg@2.42.7
      prefix: /usr
    buildable: false
  ghostscript:
    externals:
    - spec: ghostscript@9.27
      prefix: /usr
    buildable: false
  gaussian:
    permissions:
      read: group
      group: gaussian
  vasp6:
    permissions:
      read: group
      group: vasp-6
  all:
    compiler: [gcc, intel, pgi, clang, xl, nag, fj, aocc]
    providers:
      awk: [gawk]
      blas: [openblas, amdblis]
      D: [ldc]
      daal: [intel-daal]
      elf: [elfutils]
      fftw-api: [fftw, amdfftw]
      flame: [libflame, amdlibflame]
      fuse: [libfuse]
      gl: [mesa+opengl, mesa18, opengl]
      glu: [mesa-glu, openglu]
      glx: [mesa+glx, mesa18+glx, opengl]
      golang: [gcc]
      iconv: [libiconv]
      ipp: [intel-ipp]
      java: [openjdk, jdk, ibm-java]
      jpeg: [libjpeg-turbo, libjpeg]
      lapack: [openblas, amdlibflame]
      lua-lang: [lua, lua-luajit]
      mariadb-client: [mariadb-c-client, mariadb]
      mkl: [intel-mkl]
      mpe: [mpe2]
      mpi: [openmpi, mpich]
      mysql-client: [mysql, mariadb-c-client]
      opencl: [pocl]
      onedal: [intel-oneapi-dal]
      osmesa: [mesa+osmesa, mesa18+osmesa]
      pbs: [openpbs, torque]
      pil: [py-pillow]
      pkgconfig: [pkgconf, pkg-config]
      rpc: [libtirpc]
      scalapack: [netlib-scalapack, amdscalapack]
      sycl: [hipsycl]
      szip: [libaec, libszip]
      tbb: [intel-tbb]
      unwind: [libunwind]
      uuid: [util-linux-uuid, libuuid]
      xxd: [xxd-standalone, vim]
      yacc: [bison, byacc]
      ziglang: [zig]
    permissions:
      read: world
      write: user
repos:
- $spack/var/spack/repos/sdsc
- $spack/var/spack/repos/builtin
upstreams: {}
Input spec
--------------------------------
[    ]  .namd@2.14%gcc@10.2.0+cuda cuda_arch=70,80 interface=tcl
[    ]      ^builtin.charmpp@6.10.2%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +cuda~omp~papi+production~pthreads+shared~smp~syncft~tcp~tracing backend=multicore build-target=charm++ pmi=none arch=linux-rocky8-cascadelake
[bl  ]          ^builtin.cuda@11.2.2%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~dev arch=linux-rocky8-cascadelake
[bl  ]              ^builtin.libxml2@2.9.12%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~python arch=linux-rocky8-cascadelake
[bl  ]                  ^builtin.libiconv@1.16%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native"  libs=shared,static arch=linux-rocky8-cascadelake
[bl  ]                  ^builtin.xz@5.2.5%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~pic libs=shared,static arch=linux-rocky8-cascadelake
[bl  ]                  ^builtin.zlib@1.2.11%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +optimize+pic+shared arch=linux-rocky8-cascadelake
[    ]      ^builtin.fftw@3.3.10%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~mpi~openmp~pfft_patches precision=double,float arch=linux-rocky8-cascadelake
[    ]      ^sdsc.tcl@8.5.9%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native"  arch=linux-rocky8-cascadelake
[bl  ]          ^builtin.zlib@1.2.11%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +optimize+pic+shared arch=linux-rocky8-cascadelake

Concretized
--------------------------------
ruwh4yb  [    ]  builtin.namd@2.14%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +cuda cuda_arch=70,80 fftw=3 interface=tcl patches=cdcbc3846be1dfd6dbf958177c703e15ef5343e1461ed68d02be8fd0512429d5 arch=linux-rocky8-cascadelake
ldvm3k4  [bl  ]      ^builtin.charmpp@6.10.2%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +cuda~omp~papi+production~pthreads+shared~smp~syncft~tcp~tracing backend=multicore build-target=charm++ pmi=none arch=linux-rocky8-cascadelake
blza2ps  [bl  ]          ^builtin.cuda@11.2.2%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~dev arch=linux-rocky8-cascadelake
2q4yola  [bl  ]              ^builtin.libxml2@2.9.12%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~python arch=linux-rocky8-cascadelake
5a3xt3s  [bl  ]                  ^builtin.libiconv@1.16%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native"  libs=shared,static arch=linux-rocky8-cascadelake
5xho2dj  [bl  ]                  ^builtin.xz@5.2.5%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~pic libs=shared,static arch=linux-rocky8-cascadelake
2c5fvip  [bl  ]                  ^builtin.zlib@1.2.11%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" +optimize+pic+shared arch=linux-rocky8-cascadelake
7ahyh5v  [bl  ]      ^builtin.fftw@3.3.10%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native" ~mpi~openmp~pfft_patches precision=double,float arch=linux-rocky8-cascadelake
6xpusrk  [bl  ]      ^sdsc.tcl@8.5.9%gcc@10.2.0 cflags="-O2 -march=native" cxxflags="-O2 -march=native" fflags="-O2 -march=native"  arch=linux-rocky8-cascadelake

real 18.01
user 14.73
sys 2.20
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/libiconv-1.16-5a3xt3stjvynuygepnoy3fwkc4p524af
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/xz-5.2.5-5xho2djgxmtrybtbc7q5q2yi5juesbqu
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/zlib-1.2.11-2c5fvipdd5evacdfivwheqdtijr5om5z
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/fftw-3.3.10-7ahyh5v5twqbte3ye62ukz55jqgwheuk
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/libxml2-2.9.12-2q4yola2ylijts5vlbv353pptxgmu5xs
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/tcl-8.5.9-6xpusrkzpnmqhi3uw6anyn7unxlsqvty
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/cuda-11.2.2-blza2psofa3wr2zumqrnh4je2f7ze3mx
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/charmpp-6.10.2-ldvm3k4q4wfeys6rl434fpjnpt6h7aki
==> Installing namd-2.14-ruwh4yb7tvk6d2ys5l6jfkwl4y2s25ll
==> No binary for namd-2.14-ruwh4yb7tvk6d2ys5l6jfkwl4y2s25ll found: installing from source
==> Warning: Expected user 527835 to own /scratch/spack_gpu, but it is owned by 0
==> Fetching file:///home/spack_gpu/software/namd/NAMD_2.14_Source.tar.gz
==> Applied patch /cm/shared/apps/spack/0.17.3/gpu/b/var/spack/repos/builtin/packages/namd/inherited-member-2.14.patch
==> namd: Executing phase: 'edit'
==> namd: Executing phase: 'build'
==> namd: Executing phase: 'install'
==> namd: Successfully installed namd-2.14-ruwh4yb7tvk6d2ys5l6jfkwl4y2s25ll
  Fetch: 0.44s.  Build: 1m 35.78s.  Total: 1m 36.22s.
[+] /cm/shared/apps/spack/0.17.3/gpu/b/opt/spack/linux-rocky8-cascadelake/gcc-10.2.0/namd-2.14-ruwh4yb7tvk6d2ys5l6jfkwl4y2s25ll
real 115.53
user 550.74
sys 309.71
